Baseline Model - Logistic Regression
XGBoost, RandomForest
Try Feature Importance and reevaluate

https://www.kaggle.com/jiegeng94/everyone-do-this-at-the-beginning - remove skewed and null columns
https://www.kaggle.com/theoviel/load-the-totality-of-the-data - load full data by giving dtypes

Introduction
Dataset
Finds about dataset and Graphs
Baselines
Method - LGBM vs XGBoost
Results
Conclusion 

Results - 
Baseline - Accuracy =  50.05435812011717

LR - 
LR Training -  114.87810325622559s
LR Prediction -  0.8793430328369141s
Accuracy =  52.630401108438605

All columns - 
df size -  1.04GB
[15716]	training's auc: 0.736771	valid_1's auc: 0.709454
LGBM Training - 28677.198559999466s
LGBM Prediction - 1948.4909536838531s
Accuracy =  65.14976375623691

10 columns - 
df[:10] size - 352.85MB
[13575]	training's auc: 0.704034	valid_1's auc: 0.680794
LGBM Training - 18474.14462161064s
LGBM Prediction - 1682.537677526474s
Accuracy =  63.17464394782658

20 coluns - 
df[:20] size - 536.63MB
[12467]	training's auc: 0.720744	valid_1's auc: 0.698319
LGBM Training - 18156.33614730835s
LGBM Prediction - 1536.3222620487213s
Accuracy =  64.35683576063745

40 columns - 
df[:40] size - 852.72MB
[16436]	training's auc: 0.736546	valid_1's auc: 0.708078
LGBM Training - 26547.406556129456s
LGBM Prediction - 2014.1331791877747s
Accuracy =  65.08340533513922